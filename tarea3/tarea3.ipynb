{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística\n",
    "\n",
    "Como estamos hablando de Regresión Lógística ya sabemos que vamos a ver si supera la probabilidad de  que sea en la parte superior de la función (con forma de S) o la parte inferior y aunque podemos combinar varias regresiones para mejorar el resultado, ví que sklearn tiene dos enfoques uno que permite con una sola función detectar múltiples clases o solo trabajar de manera Binarizada, por eso en este trabajo solo implementé una, porque ya sabía que el modelo que mejor se iba a adaptar era la red neuronal CNN; al principo solo me daba 10%, pero utilziando HOG para detectar texturas y formas; y fourier logré aumentar en el mejor caso hasta 40%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo: 40.11%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from skimage.feature import hog\n",
    "\n",
    "# Ruta a la carpeta principal que contiene las subcarpetas\n",
    "ruta_carpeta_general = 'train/'\n",
    "\n",
    "# Listas para almacenar las imágenes y etiquetas\n",
    "imagenes = []\n",
    "etiquetas = []\n",
    "\n",
    "# Función para extraer características HOG\n",
    "def extraer_caracteristicas_hog(imagen):\n",
    "    return hog(imagen, pixels_per_cell=(16, 16), cells_per_block=(2, 2), orientations=9)\n",
    "\n",
    "# Función para extraer características Fourier\n",
    "def extraer_caracteristicas_fourier(imagen):\n",
    "    f_transformada = np.fft.fft2(imagen)\n",
    "    f_shifted = np.fft.fftshift(f_transformada)  \n",
    "    magnitud_espectro = np.abs(f_shifted)        \n",
    "    # Redimensionamos para reducir tamaño y convertimos en vector\n",
    "    magnitud_espectro = cv2.resize(magnitud_espectro, (32, 32)).flatten()\n",
    "    return magnitud_espectro\n",
    "\n",
    "# Recorrer las subcarpetas\n",
    "for subcarpeta in os.listdir(ruta_carpeta_general):\n",
    "    ruta_subcarpeta = os.path.join(ruta_carpeta_general, subcarpeta)\n",
    "    if os.path.isdir(ruta_subcarpeta):\n",
    "        for archivo in os.listdir(ruta_subcarpeta):\n",
    "            ruta_imagen = os.path.join(ruta_subcarpeta, archivo)\n",
    "            # Cargar la imagen en escala de grises y redimensionarla\n",
    "            imagen = cv2.imread(ruta_imagen, cv2.IMREAD_GRAYSCALE)\n",
    "            imagen = cv2.resize(imagen, (128, 128))\n",
    "            # Extraer características HOG y Fourier\n",
    "            caracteristicas_hog = extraer_caracteristicas_hog(imagen)\n",
    "            caracteristicas_fourier = extraer_caracteristicas_fourier(imagen)\n",
    "            # Combinar características de HOG y Fourier\n",
    "            caracteristicas_combinadas = np.concatenate([caracteristicas_hog, caracteristicas_fourier])\n",
    "            imagenes.append(caracteristicas_combinadas)\n",
    "            etiquetas.append(subcarpeta)\n",
    "\n",
    "# Convertir listas a arrays de NumPy\n",
    "X = np.array(imagenes)\n",
    "y = np.array(etiquetas)\n",
    "\n",
    "# Escalado de características antes de PCA\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Aplicar PCA para reducir la dimensionalidad\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Crear y entrenar el modelo de regresión logística\n",
    "modelo = LogisticRegression(max_iter=2000, solver='liblinear', C=0.1)\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = modelo.predict(X_test)\n",
    "precision = accuracy_score(y_test, y_pred)\n",
    "print(f'Precisión del modelo: {precision * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta a la carpeta de prueba (test)\n",
    "ruta_carpeta_test = 'test/'\n",
    "\n",
    "# Listas para almacenar las imágenes y nombres de archivo\n",
    "imagenes_test = []\n",
    "nombres_archivos = []\n",
    "\n",
    "# Función para extraer características HOG (misma que en el entrenamiento)\n",
    "def extraer_caracteristicas_hog(imagen):\n",
    "    return hog(imagen, pixels_per_cell=(16, 16), cells_per_block=(2, 2), orientations=9)\n",
    "\n",
    "# Función para extraer características Fourier (misma que en el entrenamiento)\n",
    "def extraer_caracteristicas_fourier(imagen):\n",
    "    f_transformada = np.fft.fft2(imagen)\n",
    "    f_shifted = np.fft.fftshift(f_transformada)\n",
    "    magnitud_espectro = np.abs(f_shifted)\n",
    "    magnitud_espectro = cv2.resize(magnitud_espectro, (32, 32)).flatten()\n",
    "    return magnitud_espectro\n",
    "\n",
    "# Recorrer la carpeta de prueba y ordenar los archivos alfabéticamente\n",
    "for archivo in sorted(os.listdir(ruta_carpeta_test)):\n",
    "    ruta_imagen = os.path.join(ruta_carpeta_test, archivo)\n",
    "    # Cargar la imagen en escala de grises y redimensionarla al mismo tamaño que las imágenes de entrenamiento\n",
    "    imagen = cv2.imread(ruta_imagen, cv2.IMREAD_GRAYSCALE)\n",
    "    if imagen is not None:\n",
    "        imagen = cv2.resize(imagen, (128, 128))  \n",
    "        # Extraer características HOG y Fourier\n",
    "        imagen_hog = extraer_caracteristicas_hog(imagen)\n",
    "        imagen_fourier = extraer_caracteristicas_fourier(imagen)\n",
    "        # Combinar características de HOG y Fourier\n",
    "        imagen_caracteristicas = np.concatenate([imagen_hog, imagen_fourier])\n",
    "        imagenes_test.append(imagen_caracteristicas)\n",
    "        nombres_archivos.append(archivo)\n",
    "\n",
    "# Convertir la lista de características combinadas a un array de NumPy\n",
    "X_test = np.array(imagenes_test)\n",
    "\n",
    "# Escalar las características usando el mismo escalador del entrenamiento\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reducir dimensiones con PCA\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Hacer predicciones con el modelo entrenado\n",
    "predicciones = modelo.predict(X_test_pca)\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'file': nombres_archivos,   \n",
    "    'label': predicciones       \n",
    "})\n",
    "\n",
    "# Guardar los resultados en un archivo CSV\n",
    "resultados.to_csv('Relog.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "En este proceso tuve dos inconvenientes, el primero es las librerías no me estaban funcionando y el segundo es que se me acabaron los creditos de google colab(con kaggle duré 7 horas en una sesión y no me llegó a dar resultado); pero este fue uno de los modelos que mejor se adaptó, y con lo poco que logré probar en kaggle aumentando la resolución de la imagen, con más epocas ya el acuracy en test era del 82%, así que si pudiese terminar la compilación lo más seguro es que  mejorara este resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jposa\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "C:\\Users\\jposa\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\activations\\leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\jposa\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 136ms/step - accuracy: 0.1280 - loss: 2.9036 - val_accuracy: 0.1128 - val_loss: 3.4537 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 124ms/step - accuracy: 0.2737 - loss: 2.1679 - val_accuracy: 0.1155 - val_loss: 4.1653 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 125ms/step - accuracy: 0.3247 - loss: 1.9202 - val_accuracy: 0.2274 - val_loss: 2.7129 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 125ms/step - accuracy: 0.3697 - loss: 1.7982 - val_accuracy: 0.3808 - val_loss: 1.6756 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 127ms/step - accuracy: 0.3935 - loss: 1.6820 - val_accuracy: 0.2939 - val_loss: 2.3915 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 128ms/step - accuracy: 0.4232 - loss: 1.5666 - val_accuracy: 0.2625 - val_loss: 1.9325 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 127ms/step - accuracy: 0.4483 - loss: 1.5713 - val_accuracy: 0.3595 - val_loss: 2.0280 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.4989 - loss: 1.4006 - val_accuracy: 0.4880 - val_loss: 1.3419 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.4873 - loss: 1.4029 - val_accuracy: 0.2440 - val_loss: 3.5741 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 130ms/step - accuracy: 0.5021 - loss: 1.3670 - val_accuracy: 0.4279 - val_loss: 1.4879 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 130ms/step - accuracy: 0.5433 - loss: 1.2988 - val_accuracy: 0.5518 - val_loss: 1.2377 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 128ms/step - accuracy: 0.5591 - loss: 1.2640 - val_accuracy: 0.2782 - val_loss: 3.0516 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 129ms/step - accuracy: 0.5314 - loss: 1.2911 - val_accuracy: 0.3429 - val_loss: 1.9775 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.5630 - loss: 1.1984 - val_accuracy: 0.4307 - val_loss: 1.6766 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 132ms/step - accuracy: 0.5950 - loss: 1.1675 - val_accuracy: 0.5203 - val_loss: 1.3296 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 128ms/step - accuracy: 0.5870 - loss: 1.1490 - val_accuracy: 0.3318 - val_loss: 2.1741 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 129ms/step - accuracy: 0.6050 - loss: 1.1093 - val_accuracy: 0.3549 - val_loss: 1.8796 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 129ms/step - accuracy: 0.6282 - loss: 1.0447 - val_accuracy: 0.5628 - val_loss: 1.3646 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 132ms/step - accuracy: 0.6385 - loss: 0.9833 - val_accuracy: 0.5582 - val_loss: 1.2097 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.6393 - loss: 0.9741 - val_accuracy: 0.6248 - val_loss: 1.0241 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.6544 - loss: 0.9687 - val_accuracy: 0.6756 - val_loss: 0.8572 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.6489 - loss: 0.9670 - val_accuracy: 0.5813 - val_loss: 1.1709 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 134ms/step - accuracy: 0.6579 - loss: 0.9558 - val_accuracy: 0.6608 - val_loss: 0.8848 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 141ms/step - accuracy: 0.6837 - loss: 0.9220 - val_accuracy: 0.3623 - val_loss: 1.9249 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 133ms/step - accuracy: 0.6490 - loss: 0.9519 - val_accuracy: 0.6774 - val_loss: 0.8211 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 131ms/step - accuracy: 0.6803 - loss: 0.8739 - val_accuracy: 0.5915 - val_loss: 1.0816 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 135ms/step - accuracy: 0.6934 - loss: 0.8609 - val_accuracy: 0.5582 - val_loss: 1.2519 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 132ms/step - accuracy: 0.6918 - loss: 0.8482 - val_accuracy: 0.5739 - val_loss: 1.1360 - learning_rate: 5.0000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 123ms/step - accuracy: 0.6970 - loss: 0.8446 - val_accuracy: 0.4233 - val_loss: 1.6921 - learning_rate: 5.0000e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 115ms/step - accuracy: 0.7197 - loss: 0.8125 - val_accuracy: 0.6636 - val_loss: 0.8853 - learning_rate: 5.0000e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.7180 - loss: 0.7756 - val_accuracy: 0.7403 - val_loss: 0.6560 - learning_rate: 2.5000e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 111ms/step - accuracy: 0.7293 - loss: 0.7289 - val_accuracy: 0.6969 - val_loss: 0.7677 - learning_rate: 2.5000e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 113ms/step - accuracy: 0.7432 - loss: 0.7404 - val_accuracy: 0.7311 - val_loss: 0.7009 - learning_rate: 2.5000e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 113ms/step - accuracy: 0.7268 - loss: 0.7352 - val_accuracy: 0.6821 - val_loss: 0.8945 - learning_rate: 2.5000e-04\n",
      "Epoch 35/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.7335 - loss: 0.7256 - val_accuracy: 0.6858 - val_loss: 0.8773 - learning_rate: 2.5000e-04\n",
      "Epoch 36/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.7368 - loss: 0.7432 - val_accuracy: 0.7514 - val_loss: 0.5950 - learning_rate: 2.5000e-04\n",
      "Epoch 37/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.7474 - loss: 0.6804 - val_accuracy: 0.6774 - val_loss: 0.9268 - learning_rate: 2.5000e-04\n",
      "Epoch 38/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 115ms/step - accuracy: 0.7325 - loss: 0.7784 - val_accuracy: 0.7551 - val_loss: 0.7093 - learning_rate: 2.5000e-04\n",
      "Epoch 39/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.7400 - loss: 0.7118 - val_accuracy: 0.7837 - val_loss: 0.5418 - learning_rate: 2.5000e-04\n",
      "Epoch 40/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 113ms/step - accuracy: 0.7529 - loss: 0.6705 - val_accuracy: 0.6044 - val_loss: 1.1317 - learning_rate: 2.5000e-04\n",
      "Epoch 41/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 118ms/step - accuracy: 0.7349 - loss: 0.7284 - val_accuracy: 0.7597 - val_loss: 0.6327 - learning_rate: 2.5000e-04\n",
      "Epoch 42/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 115ms/step - accuracy: 0.7504 - loss: 0.6990 - val_accuracy: 0.7301 - val_loss: 0.7222 - learning_rate: 2.5000e-04\n",
      "Epoch 43/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.7440 - loss: 0.6719 - val_accuracy: 0.7736 - val_loss: 0.5716 - learning_rate: 2.5000e-04\n",
      "Epoch 44/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 114ms/step - accuracy: 0.7511 - loss: 0.6665 - val_accuracy: 0.7893 - val_loss: 0.5665 - learning_rate: 2.5000e-04\n",
      "Epoch 45/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 113ms/step - accuracy: 0.7642 - loss: 0.6265 - val_accuracy: 0.7643 - val_loss: 0.6245 - learning_rate: 1.2500e-04\n",
      "Epoch 46/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.7689 - loss: 0.6256 - val_accuracy: 0.7292 - val_loss: 0.6416 - learning_rate: 1.2500e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 112ms/step - accuracy: 0.7566 - loss: 0.6534 - val_accuracy: 0.7227 - val_loss: 0.7182 - learning_rate: 1.2500e-04\n",
      "Epoch 48/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 113ms/step - accuracy: 0.7628 - loss: 0.6243 - val_accuracy: 0.7301 - val_loss: 0.7292 - learning_rate: 1.2500e-04\n",
      "Epoch 49/50\n",
      "\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 113ms/step - accuracy: 0.7713 - loss: 0.6026 - val_accuracy: 0.7237 - val_loss: 0.7301 - learning_rate: 1.2500e-04\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step\n",
      "Reporte de clasificación:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "              Black-grass       0.52      0.44      0.48        59\n",
      "                 Charlock       0.71      0.95      0.81        87\n",
      "                 Cleavers       0.88      0.67      0.76        57\n",
      "         Common Chickweed       0.93      0.96      0.95       120\n",
      "             Common wheat       0.82      0.88      0.84        56\n",
      "                  Fat Hen       0.91      0.73      0.81        99\n",
      "         Loose Silky-bent       0.75      0.82      0.78       122\n",
      "                    Maize       0.89      0.95      0.92        41\n",
      "        Scentless Mayweed       0.89      0.90      0.90       113\n",
      "          Shepherds Purse       0.79      0.63      0.70        43\n",
      "Small-flowered Cranesbill       0.96      0.93      0.95        88\n",
      "               Sugar beet       0.80      0.84      0.82        79\n",
      "                      cat       0.67      0.63      0.65        35\n",
      "                      cow       0.34      0.85      0.48        26\n",
      "                      dog       0.25      0.13      0.17        30\n",
      "                    horse       0.25      0.04      0.06        27\n",
      "\n",
      "                 accuracy                           0.78      1082\n",
      "                macro avg       0.71      0.71      0.69      1082\n",
      "             weighted avg       0.78      0.78      0.78      1082\n",
      "\n",
      "Matriz de confusión:\n",
      " [[ 26   0   0   0   0   0  29   0   1   0   0   2   1   0   0   0]\n",
      " [  0  83   3   0   0   0   0   0   0   1   0   0   0   0   0   0]\n",
      " [  0  18  38   0   0   1   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 115   0   1   1   0   1   1   1   0   0   0   0   0]\n",
      " [  5   0   0   0  49   1   0   1   0   0   0   0   0   0   0   0]\n",
      " [  0   9   2   1   3  72   0   0   1   0   0  11   0   0   0   0]\n",
      " [ 19   0   0   0   1   0 100   0   1   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0   0   0   1  39   0   0   0   1   0   0   0   0]\n",
      " [  0   2   0   1   0   0   2   1 102   4   0   1   0   0   0   0]\n",
      " [  0   2   0   4   0   1   0   0   7  27   2   0   0   0   0   0]\n",
      " [  0   2   0   1   2   0   0   0   0   1  82   0   0   0   0   0]\n",
      " [  0   1   0   1   5   3   0   2   1   0   0  66   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   1   0   0   0   0  22   4   7   1]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1  22   3   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   7  17   4   2]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   2  22   2   1]]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Configuración inicial\n",
    "img_height, img_width = 32, 32  \n",
    "ruta_carpeta_general = 'train/'\n",
    "\n",
    "# Listas para almacenar imágenes y etiquetas\n",
    "imagenes = []\n",
    "etiquetas = []\n",
    "\n",
    "# Cargar imágenes y etiquetas desde las subcarpetas\n",
    "for subcarpeta in os.listdir(ruta_carpeta_general):\n",
    "    ruta_subcarpeta = os.path.join(ruta_carpeta_general, subcarpeta)\n",
    "    if os.path.isdir(ruta_subcarpeta):\n",
    "        for archivo in os.listdir(ruta_subcarpeta):\n",
    "            ruta_imagen = os.path.join(ruta_subcarpeta, archivo)\n",
    "            imagen = cv2.imread(ruta_imagen)\n",
    "            imagen = cv2.resize(imagen, (img_height, img_width))\n",
    "            imagenes.append(imagen)\n",
    "            etiquetas.append(subcarpeta)\n",
    "\n",
    "# Convertir listas a arrays y normalizar\n",
    "X = np.array(imagenes) / 255.0  \n",
    "y = np.array(etiquetas)\n",
    "\n",
    "# Binarización de las etiquetas\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(y)\n",
    "num_classes = len(lb.classes_)\n",
    "\n",
    "# División de datos en entrenamiento y prueba\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Aumento de datos\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Definir el modelo Sequential con capas convolucionales\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), padding='same', input_shape=(img_height, img_width, 3)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Conv2D(32, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Conv2D(64, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(128, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.Conv2D(128, (3, 3), padding='same'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(alpha=0.01),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks para evitar sobreajuste y reducir la tasa de aprendizaje\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-5)\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluación del modelo en el conjunto de prueba\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Imprimir la matriz de confusión y el reporte de clasificación\n",
    "print(\"Reporte de clasificación:\\n\", classification_report(y_true_classes, y_pred_classes, target_names=lb.classes_))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_true_classes, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicciones guardadas en CNN_test.csv\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas necesarias\n",
    "import pandas as pd\n",
    "\n",
    "# Ruta de la carpeta de prueba\n",
    "ruta_carpeta_test = 'test/'\n",
    "\n",
    "# Listas para almacenar nombres de archivos y predicciones\n",
    "nombres_archivos = []\n",
    "predicciones = []\n",
    "\n",
    "# Cargar imágenes desde la carpeta de prueba\n",
    "for archivo in os.listdir(ruta_carpeta_test):\n",
    "    ruta_imagen = os.path.join(ruta_carpeta_test, archivo)\n",
    "    imagen = cv2.imread(ruta_imagen)\n",
    "    if imagen is not None:\n",
    "        imagen = cv2.resize(imagen, (img_height, img_width))\n",
    "        imagen = np.array(imagen) / 255.0  \n",
    "        imagen = np.expand_dims(imagen, axis=0)  \n",
    "\n",
    "        # Realizar la predicción\n",
    "        prediccion = model.predict(imagen, verbose=0)\n",
    "        clase_predicha = np.argmax(prediccion, axis=1)[0]  \n",
    "        predicciones.append(clase_predicha)  \n",
    "        nombres_archivos.append(archivo)  \n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados\n",
    "resultados = pd.DataFrame({\n",
    "    'file': nombres_archivos,   \n",
    "    'label': lb.classes_[predicciones]  \n",
    "})\n",
    "\n",
    "# Guardar los resultados en un archivo CSV\n",
    "resultados.to_csv('CNN_test.csv', index=False)\n",
    "\n",
    "print(\"Predicciones guardadas en CNN_test.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
